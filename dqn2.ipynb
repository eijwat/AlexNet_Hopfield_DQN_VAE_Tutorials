{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOC2MUoaoTZiyFXPV0RurRW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **DQN with CartPole**"],"metadata":{"id":"fHX0jm1jZ6HY"}},{"cell_type":"markdown","source":["本コードはCartPole向けの最小限実装サンプルです。実行するにはランタイムはT4 GPU以上を選択してください（実行時間は１０分ほど）。CartPoleを倒さないように学習が進められ、学習が進むにつれてDurationが長くなっていきます。CartPoleのアニメーションを表示させると直感的に分かりやすいですが、Colabでは無理があるので断念しました。このコードをベースに Double DQN や Prioritized Replay など拡張手法を試すこともできますので、書籍やチュートリアルでの応用例としても活用しやすい内容だと思います。ぜひご活用ください。"],"metadata":{"id":"7F-jLeTCZizg"}},{"cell_type":"code","source":["!pip install gymnasium[classic_control]"],"metadata":{"id":"BaM59b-Dv_PZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gX2gWw9HvYFf"},"outputs":[],"source":["import gymnasium as gym\n","import math\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple, deque\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","env = gym.make(\"CartPole-v1\")\n","\n","# set up matplotlib\n","is_ipython = 'inline' in matplotlib.get_backend()\n","if is_ipython:\n","    from IPython import display\n","\n","plt.ion()\n","\n","# if GPU is to be used\n","device = torch.device(\n","    \"cuda\" if torch.cuda.is_available() else\n","    \"mps\" if torch.backends.mps.is_available() else\n","    \"cpu\"\n",")\n","\n","Transition = namedtuple('Transition',\n","                        ('state', 'action', 'next_state', 'reward'))\n","\n","\n","class ReplayMemory(object):\n","\n","    def __init__(self, capacity):\n","        self.memory = deque([], maxlen=capacity)\n","\n","    def push(self, *args):\n","        \"\"\"Save a transition\"\"\"\n","        self.memory.append(Transition(*args))\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","\n","class DQN(nn.Module):\n","\n","    def __init__(self, n_observations, n_actions):\n","        super(DQN, self).__init__()\n","        self.layer1 = nn.Linear(n_observations, 128)\n","        self.layer2 = nn.Linear(128, 128)\n","        self.layer3 = nn.Linear(128, n_actions)\n","\n","    # Called with either one element to determine next action, or a batch\n","    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n","    def forward(self, x):\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        return self.layer3(x)\n","\n","# BATCH_SIZE is the number of transitions sampled from the replay buffer\n","# GAMMA is the discount factor as mentioned in the previous section\n","# EPS_START is the starting value of epsilon\n","# EPS_END is the final value of epsilon\n","# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n","# TAU is the update rate of the target network\n","# LR is the learning rate of the ``AdamW`` optimizer\n","BATCH_SIZE = 128\n","GAMMA = 0.99\n","EPS_START = 0.9\n","EPS_END = 0.05\n","EPS_DECAY = 1000\n","TAU = 0.005\n","LR = 1e-4\n","\n","# Get number of actions from gym action space\n","n_actions = env.action_space.n\n","# Get the number of state observations\n","state, info = env.reset()\n","n_observations = len(state)\n","\n","policy_net = DQN(n_observations, n_actions).to(device)\n","target_net = DQN(n_observations, n_actions).to(device)\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n","memory = ReplayMemory(10000)\n","\n","\n","steps_done = 0\n","\n","\n","def select_action(state):\n","    global steps_done\n","    sample = random.random()\n","    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n","        math.exp(-1. * steps_done / EPS_DECAY)\n","    steps_done += 1\n","    if sample > eps_threshold:\n","        with torch.no_grad():\n","            # t.max(1) will return the largest column value of each row.\n","            # second column on max result is index of where max element was\n","            # found, so we pick action with the larger expected reward.\n","            return policy_net(state).max(1).indices.view(1, 1)\n","    else:\n","        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n","\n","\n","episode_durations = []\n","\n","\n","def plot_durations(show_result=False):\n","    plt.figure(1)\n","    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n","    if show_result:\n","        plt.title('Result')\n","    else:\n","        plt.clf()\n","        plt.title('Training...')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Duration')\n","    plt.plot(durations_t.numpy())\n","    # Take 100 episode averages and plot them too\n","    if len(durations_t) >= 100:\n","        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n","        means = torch.cat((torch.zeros(99), means))\n","        plt.plot(means.numpy())\n","\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    if is_ipython:\n","        if not show_result:\n","            display.display(plt.gcf())\n","            display.clear_output(wait=True)\n","        else:\n","            display.display(plt.gcf())\n","\n","\n","def optimize_model():\n","    if len(memory) < BATCH_SIZE:\n","        return\n","    transitions = memory.sample(BATCH_SIZE)\n","    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n","    # detailed explanation). This converts batch-array of Transitions\n","    # to Transition of batch-arrays.\n","    batch = Transition(*zip(*transitions))\n","\n","    # Compute a mask of non-final states and concatenate the batch elements\n","    # (a final state would've been the one after which simulation ended)\n","    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n","                                          batch.next_state)), device=device, dtype=torch.bool)\n","    non_final_next_states = torch.cat([s for s in batch.next_state\n","                                                if s is not None])\n","    state_batch = torch.cat(batch.state)\n","    action_batch = torch.cat(batch.action)\n","    reward_batch = torch.cat(batch.reward)\n","\n","    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n","    # columns of actions taken. These are the actions which would've been taken\n","    # for each batch state according to policy_net\n","    state_action_values = policy_net(state_batch).gather(1, action_batch)\n","\n","    # Compute V(s_{t+1}) for all next states.\n","    # Expected values of actions for non_final_next_states are computed based\n","    # on the \"older\" target_net; selecting their best reward with max(1).values\n","    # This is merged based on the mask, such that we'll have either the expected\n","    # state value or 0 in case the state was final.\n","    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n","    with torch.no_grad():\n","        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n","    # Compute the expected Q values\n","    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n","\n","    # Compute Huber loss\n","    criterion = nn.SmoothL1Loss()\n","    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n","\n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","    # In-place gradient clipping\n","    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n","    optimizer.step()\n","\n","\n","if torch.cuda.is_available() or torch.backends.mps.is_available():\n","    num_episodes = 600\n","else:\n","    num_episodes = 50\n","\n","\n","for i_episode in range(num_episodes):\n","    # Initialize the environment and get its state\n","    state, info = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n","    for t in count():\n","        action = select_action(state)\n","        observation, reward, terminated, truncated, _ = env.step(action.item())\n","        reward = torch.tensor([reward], device=device)\n","        done = terminated or truncated\n","\n","        if terminated:\n","            next_state = None\n","        else:\n","            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n","\n","        # Store the transition in memory\n","        memory.push(state, action, next_state, reward)\n","\n","        # Move to the next state\n","        state = next_state\n","\n","        # Perform one step of the optimization (on the policy network)\n","        optimize_model()\n","\n","        # Soft update of the target network's weights\n","        # θ′ ← τ θ + (1 −τ )θ′\n","        target_net_state_dict = target_net.state_dict()\n","        policy_net_state_dict = policy_net.state_dict()\n","        for key in policy_net_state_dict:\n","            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n","        target_net.load_state_dict(target_net_state_dict)\n","\n","        if done:\n","            episode_durations.append(t + 1)\n","            plot_durations()\n","            break\n","\n","print('Complete')\n","plot_durations(show_result=True)\n","plt.ioff()\n","plt.show()\n"]},{"cell_type":"markdown","source":["Thanks to\n","*   https://qiita.com/kongo-jun/items/f44994d1196d525525be\n","*   https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"],"metadata":{"id":"hkqUJwtFZuSv"}}]}