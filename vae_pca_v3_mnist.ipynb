{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Lxlf1qCkTe3z1MrcvEGMllPgaPZrCDYm","timestamp":1736508237647},{"file_id":"1ncBIIjHGlZrinfu3LeS9XlX-ECZDm0ZG","timestamp":1736488376989},{"file_id":"13LIET9UTrgUPnPO383I306G3GOzelppQ","timestamp":1736485072737},{"file_id":"1zmCLziyLqmkzo2itOu6I_rSN9vRfAsFp","timestamp":1736465895244},{"file_id":"1N6wUxLd1S_oOrs0eIzNI0dmZGNcDAzak","timestamp":1736326412808}],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPUUJ2KhaxEZDICCHFBDMor"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **VAE & PCA with MNIST**"],"metadata":{"id":"gAaM430oeHUn"}},{"cell_type":"markdown","source":["全結合を用いたシンプルな VAE で MNIST を学習、損失は BCE+KL、潜在次元 2 で可視化と生成にフォーカス。分かりやすいサンプルとして仕上げました。全体としては、「潜在空間が 2 次元だとラベルごとにどのように分布するか」「平均ベクトルや補間でどんな画像が出力されるか」など、VAE を使った可視化の定番を丁寧に押さえているチュートリアルコードです。ランタイムはT4 GPU以上を選択してください。"],"metadata":{"id":"W_9htME6eHII"}},{"cell_type":"markdown","source":["# **モジュールのimport**"],"metadata":{"id":"wHeFvMbD1kuL"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST, FashionMNIST\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from matplotlib import animation, rc\n","\n","!apt install imagemagick # gif作成に必要"],"metadata":{"id":"IineRh_ixxge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **データセットの作成**"],"metadata":{"id":"lieTC9fb1sIa"}},{"cell_type":"code","source":["BATCH_SIZE = 100\n","\n","trainval_data = MNIST(\"./data\",\n","                   train=True,\n","                   download=True,\n","                   transform=transforms.ToTensor())\n","\n","train_size = int(len(trainval_data) * 0.8)\n","val_size = int(len(trainval_data) * 0.2)\n","train_data, val_data = torch.utils.data.random_split(trainval_data, [train_size, val_size])\n","\n","train_loader = DataLoader(dataset=train_data,\n","                          batch_size=BATCH_SIZE,\n","                          shuffle=True,\n","                          num_workers=0)\n","\n","val_loader = DataLoader(dataset=val_data,\n","                        batch_size=BATCH_SIZE,\n","                        shuffle=True,\n","                        num_workers=0)\n","\n","print(\"train data size: \",len(train_data))   #train data size:  48000\n","print(\"train iteration number: \",len(train_data)//BATCH_SIZE)   #train iteration number:  480\n","print(\"val data size: \",len(val_data))   #val data size:  12000\n","print(\"val iteration number: \",len(val_data)//BATCH_SIZE)   #val iteration number:  120"],"metadata":{"id":"WalqMSYJx0-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **データの内容確認**"],"metadata":{"id":"jDJ6b2en1ymS"}},{"cell_type":"code","source":["images, labels = next(iter(train_loader))\n","print(\"images_size:\",images.size())   #images_size: torch.Size([100, 1, 28, 28])\n","print(\"label:\",labels[:10])   #label: tensor([7, 6, 0, 6, 4, 8, 5, 2, 2, 3])\n","\n","image_numpy = images.detach().numpy().copy()\n","plt.imshow(image_numpy[0,0,:,:], cmap='gray')\n"],"metadata":{"id":"6J7UyQJRyDyl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **モデル作成と学習**"],"metadata":{"id":"pkccZLO72Fyb"}},{"cell_type":"code","source":["#モデル作成\n","class Encoder(nn.Module):\n","  def __init__(self, z_dim):\n","    super().__init__()\n","    self.lr = nn.Linear(28*28, 300)\n","    self.lr2 = nn.Linear(300, 100)\n","    self.lr_ave = nn.Linear(100, z_dim)   #average\n","    self.lr_dev = nn.Linear(100, z_dim)   #log(sigma^2)\n","    self.relu = nn.ReLU()\n","\n","  def forward(self, x):\n","    x = self.lr(x)\n","    x = self.relu(x)\n","    x = self.lr2(x)\n","    x = self.relu(x)\n","    ave = self.lr_ave(x)    #average\n","    log_dev = self.lr_dev(x)    #log(sigma^2)\n","\n","    ep = torch.randn_like(ave)   #平均0分散1の正規分布に従い生成されるz_dim次元の乱数\n","    z = ave + torch.exp(log_dev / 2) * ep   #再パラメータ化トリック\n","    return z, ave, log_dev\n","\n","class Decoder(nn.Module):\n","  def __init__(self, z_dim):\n","    super().__init__()\n","    self.lr = nn.Linear(z_dim, 100)\n","    self.lr2 = nn.Linear(100, 300)\n","    self.lr3 = nn.Linear(300, 28*28)\n","    self.relu = nn.ReLU()\n","\n","  def forward(self, z):\n","    x = self.lr(z)\n","    x = self.relu(x)\n","    x = self.lr2(x)\n","    x = self.relu(x)\n","    x = self.lr3(x)\n","    x = torch.sigmoid(x)   #MNISTのピクセル値の分布はベルヌーイ分布に近いと考えられるので、シグモイド関数を適用します。\n","    return x\n","\n","class VAE(nn.Module):\n","  def __init__(self, z_dim):\n","    super().__init__()\n","    self.encoder = Encoder(z_dim)\n","    self.decoder = Decoder(z_dim)\n","\n","  def forward(self, x):\n","    z, ave, log_dev = self.encoder(x)\n","    x = self.decoder(z)\n","    return x, z, ave, log_dev\n","\n","#損失関数\n","def criterion(predict, target, ave, log_dev):\n","  bce_loss = F.binary_cross_entropy(predict, target, reduction='sum')\n","  kl_loss = -0.5 * torch.sum(1 + log_dev - ave**2 - log_dev.exp())\n","  loss = bce_loss + kl_loss\n","  return loss\n","\n","#学習\n","z_dim = 2 #マッピングのため\n","num_epochs = 20\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = VAE(z_dim).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15], gamma=0.1)\n","\n","history = {\"train_loss\": [], \"val_loss\": [], \"ave\": [], \"log_dev\": [], \"z\": [], \"labels\":[]}\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","  for i, (x, labels) in enumerate(train_loader):\n","    input = x.to(device).view(-1, 28*28).to(torch.float32)\n","    output, z, ave, log_dev = model(input)\n","\n","    history[\"ave\"].append(ave)\n","    history[\"log_dev\"].append(log_dev)\n","    history[\"z\"].append(z)\n","    history[\"labels\"].append(labels)\n","    loss = criterion(output, input, ave, log_dev)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (i+1) % 50 == 0:\n","      print(f'Epoch: {epoch+1}, loss: {loss: 0.4f}')\n","    history[\"train_loss\"].append(loss)\n","\n","  model.eval()\n","  with torch.no_grad():\n","    for i, (x, labels) in enumerate(val_loader):\n","      input = x.to(device).view(-1, 28*28).to(torch.float32)\n","      output, z, ave, log_dev = model(input)\n","\n","      loss = criterion(output, input, ave, log_dev)\n","      history[\"val_loss\"].append(loss)\n","\n","    print(f'Epoch: {epoch+1}, val_loss: {loss: 0.4f}')\n","\n","  scheduler.step()"],"metadata":{"id":"8cAoEVyryGul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Train loss graph**"],"metadata":{"id":"Ij_39HPd2V6D"}},{"cell_type":"code","source":["train_loss_tensor = torch.stack(history[\"train_loss\"])\n","train_loss_np = train_loss_tensor.to('cpu').detach().numpy().copy()\n","plt.plot(train_loss_np, label='Train Loss', color='red', linestyle='-')"],"metadata":{"id":"30TgCtPRyOj1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Validation loss graph**"],"metadata":{"id":"YvKn_EKy2leT"}},{"cell_type":"code","source":["val_loss_tensor = torch.stack(history[\"val_loss\"])\n","val_loss_np = val_loss_tensor.to('cpu').detach().numpy().copy()\n","plt.plot(val_loss_np, label='Validation Loss', color='green', linestyle='--')"],"metadata":{"id":"1QCemnVyyTh1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **学習序盤の潜在変数の分布をプロット**"],"metadata":{"id":"Z760x32x22I0"}},{"cell_type":"code","source":["#各学習パラメータの履歴をnumpy配列に変換\n","\n","ave_tensor = torch.stack(history[\"ave\"])\n","log_var_tensor = torch.stack(history[\"log_dev\"])\n","z_tensor = torch.stack(history[\"z\"])\n","labels_tensor = torch.stack(history[\"labels\"])\n","print(ave_tensor.size())   #torch.Size([9600, 100, 2])\n","print(log_var_tensor.size())   #torch.Size([9600, 100, 2])\n","print(z_tensor.size())   #torch.Size([9600, 100, 2])\n","print(labels_tensor.size())   #torch.Size([9600, 100])\n","\n","ave_np = ave_tensor.to('cpu').detach().numpy().copy()\n","log_var_np = log_var_tensor.to('cpu').detach().numpy().copy()\n","z_np = z_tensor.to('cpu').detach().numpy().copy()\n","labels_np = labels_tensor.to('cpu').detach().numpy().copy()\n","print(ave_np.shape)   #(9600, 100, 2)\n","print(log_var_np.shape)   #(9600, 100, 2)\n","print(z_np.shape)   #(9600, 100, 2)\n","print(labels_np.shape)   #(9600, 100)\n","\n","#学習序盤の潜在変数の分布をプロット\n","map_keyword = \"tab10\"\n","cmap = plt.get_cmap(map_keyword)\n","\n","batch_num =10\n","plt.figure(figsize=[10,10])\n","for label in range(10):\n","  x = z_np[:batch_num,:,0][labels_np[:batch_num,:] == label]\n","  y = z_np[:batch_num,:,1][labels_np[:batch_num,:] == label]\n","  plt.scatter(x, y, color=cmap(label/9), label=label, s=15)\n","  plt.annotate(label, xy=(np.mean(x),np.mean(y)),size=20,color=\"black\")\n","plt.legend(loc=\"upper left\")"],"metadata":{"id":"_ca_4mT2yZ9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **学習終盤の潜在変数の分布をプロット**"],"metadata":{"id":"OoTe4JEN285s"}},{"cell_type":"code","source":["batch_num = 9580\n","plt.figure(figsize=[10,10])\n","for label in range(10):\n","  x = z_np[batch_num:,:,0][labels_np[batch_num:,:] == label]\n","  y = z_np[batch_num:,:,1][labels_np[batch_num:,:] == label]\n","  # Call cmap with label/9 to get the color\n","  plt.scatter(x, y, color=cmap(label/9), label=label, s=15)\n","  plt.annotate(label, xy=(np.mean(x),np.mean(y)),size=20,color=\"black\")\n","plt.legend(loc=\"upper left\")"],"metadata":{"id":"_VOUNI2Jye9u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **0の平均画像を生成**"],"metadata":{"id":"YgtN0awB3CZ0"}},{"cell_type":"code","source":["model.to(\"cpu\")\n","\n","label = 0\n","x_zero_mean = np.mean(ave_np[batch_num:,:,0][labels_np[batch_num:,:] == label])   #x軸の平均値\n","y_zero_mean = np.mean(ave_np[batch_num:,:,1][labels_np[batch_num:,:] == label])   #y軸の平均値\n","z_zero = torch.tensor([x_zero_mean,y_zero_mean], dtype = torch.float32)\n","\n","output = model.decoder(z_zero)\n","np_output = output.to('cpu').detach().numpy().copy()\n","np_image = np.reshape(np_output, (28, 28))\n","plt.imshow(np_image, cmap='gray')"],"metadata":{"id":"h8e3fJl3yg01"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **1の平均画像を生成**"],"metadata":{"id":"qObjQdP63FYM"}},{"cell_type":"code","source":["label = 1\n","x_one_mean = np.mean(ave_np[batch_num:,:,0][labels_np[batch_num:,:] == label])   #x軸の平均値\n","y_one_mean = np.mean(ave_np[batch_num:,:,1][labels_np[batch_num:,:] == label])   #y軸の平均値\n","z_one = torch.tensor([x_one_mean,y_one_mean], dtype = torch.float32)\n","\n","output = model.decoder(z_one)\n","np_output = output.to('cpu').detach().numpy().copy()\n","np_image = np.reshape(np_output, (28, 28))\n","plt.imshow(np_image, cmap='gray')"],"metadata":{"id":"ROVAnZf708Ip"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **0から1まで潜在変数を移動する際のアニメーション**"],"metadata":{"id":"ppPd0AZ53OiM"}},{"cell_type":"code","source":["def plot(frame):\n","    plt.cla()\n","    z_zerotoone = ((99 - frame) * z_zero +  frame * z_one) / 99\n","    output = model.decoder(z_zerotoone)\n","    np_output = output.detach().numpy().copy()\n","    np_image = np.reshape(np_output, (28, 28))\n","    plt.imshow(np_image, cmap='gray')\n","    plt.xticks([]);plt.yticks([])\n","    plt.title(\"frame={}\".format(frame))\n","\n","fig = plt.figure(figsize=(4,4))\n","anigif = animation.FuncAnimation(fig, plot, frames=99, interval=100)\n","rc('animation', html='jshtml')\n","anigif"],"metadata":{"id":"5M7WU6tUyrId"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **PCAの実装とUMAP Visualization**"],"metadata":{"id":"iPtKOx5U-Ugg"}},{"cell_type":"code","source":["# PCAの実装とUMAP Visualization\n","\n","!pip install umap-learn\n","import umap\n","from sklearn.decomposition import PCA\n","\n","test_data = MNIST(\"./data\",\n","                   train=False,  # Use train=False for test data\n","                   download=True,\n","                   transform=transforms.ToTensor())\n","\n","test_loader = DataLoader(dataset=test_data,\n","                        batch_size=BATCH_SIZE,\n","                        shuffle=False,  # Generally, no need to shuffle test data\n","                        num_workers=0)\n","\n","# 特徴量の抽出 (単純化のため、画像を1次元にフラット化)\n","features, labels = [], []\n","for x, y in test_loader:  # Now test_loader is defined\n","    x_flat = x.view(x.size(0), -1).numpy()\n","    features.append(x_flat)\n","    labels.append(y.numpy())\n","\n","features = np.concatenate(features)\n","labels = np.concatenate(labels)\n","\n","# PCAを用いた次元削減\n","pca = PCA(n_components=50)\n","pca_features = pca.fit_transform(features)\n","\n","# UMAPによる可視化\n","embedding = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean').fit_transform(pca_features)\n","\n","# 各クラスの色を定義\n","colors = [\n","    'red', 'blue', 'green', 'purple', 'orange', 'cyan', 'magenta', 'yellow', 'brown', 'pink'\n","]\n","\n","# グラフの描画\n","plt.figure(figsize=(8, 8))\n","for i in range(10):\n","    idx = labels == i\n","    plt.scatter(embedding[idx, 0], embedding[idx, 1], s=5, color=colors[i], label=f'Class {i}')\n","\n","plt.title('UMAP projection of MNIST features using PCA')\n","plt.legend()\n","plt.savefig('pca_umap.png')  # グラフを保存\n","plt.show()\n","\n","#各ラベル番号は以下の元の番号を意味する。\n"],"metadata":{"id":"fdwMKrU685f-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Thanks to\n","*  https://panhouse.blog/paper/implementation/vae-clustering/\n","*  https://qiita.com/gensal/items/613d04b5ff50b6413aa0\n","*  https://jun2life.com/2022/01/23/%E6%AC%A1%E5%85%83%E5%9C%A7%E7%B8%AE%E3%81%AE%E6%AF%94%E8%BC%83%E5%AE%9F%E8%A3%85%E3%81%A8%E5%8F%AF%E8%A6%96%E5%8C%96/\n","*  https://github.com/lmcinnes/umap/issues/113"],"metadata":{"id":"jtdrZ8yDg7H5"}}]}